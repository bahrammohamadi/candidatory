import os
import asyncio
import feedparser
import requests
import hashlib
from datetime import datetime, timedelta, timezone
from telegram import Bot, LinkPreviewOptions

async def main(event=None, context=None):
    print("[INFO] Ø´Ø±ÙˆØ¹")
    token = os.environ.get('TELEGRAM_BOT_TOKEN')
    chat_id = os.environ.get('TELEGRAM_CHANNEL_ID')
    endpoint = os.environ.get('APPWRITE_ENDPOINT', 'https://cloud.appwrite.io/v1')
    project = os.environ.get('APPWRITE_PROJECT_ID')
    key = os.environ.get('APPWRITE_API_KEY')
    database_id = os.environ.get('APPWRITE_DATABASE_ID')
    collection_id = 'history'
    if not all([token, chat_id, endpoint, project, key, database_id]):
        print("[ERROR] Ù…ØªØºÛŒØ±Ù‡Ø§ÛŒ Ù…Ø­ÛŒØ·ÛŒ Ù†Ø§Ù‚Øµ")
        return {"status": "error"}
    bot = Bot(token=token)
    headers = {
        'Content-Type': 'application/json',
        'X-Appwrite-Project': project,
        'X-Appwrite-Key': key,
    }
    rss_feeds = [
        "https://www.farsnews.ir/rss",
        "https://www.entekhab.ir/fa/rss/allnews",
        "https://www.isna.ir/rss",
        "https://www.tasnimnews.com/fa/rss/feed/0/0/0",
        "https://www.mehrnews.com/rss",
    ]
    now = datetime.now(timezone.utc)
    time_threshold = now - timedelta(hours=24)
    posted = False
    for url in rss_feeds:
        if posted:
            break
        try:
            feed = feedparser.parse(url)
            if not feed.entries:
                continue
            for entry in feed.entries:
                if posted:
                    break
                published = entry.get('published_parsed') or entry.get('updated_parsed')
                if not published:
                    continue
                pub_date = datetime(*published[:6], tzinfo=timezone.utc)
                if pub_date < time_threshold:
                    continue
                title = (entry.title or "").strip()
                link = (entry.link or "").strip()
                if not title or not link:
                    continue
                description = (entry.get('summary') or entry.get('description') or "").strip()
                # Ø³Ø§Ø®Øª hash Ø¨Ø±Ø§ÛŒ ØªØ´Ø®ÛŒØµ Ù…Ø­ØªÙˆØ§ÛŒ Ù…Ø´Ø§Ø¨Ù‡
                content_for_hash = (title.lower().strip() + " " + description[:150].lower().strip())
                content_hash = hashlib.sha256(content_for_hash.encode('utf-8')).hexdigest()
                # Ú†Ú© ØªÚ©Ø±Ø§Ø±ÛŒ (Ù„ÛŒÙ†Ú© ÛŒØ§ hash)
                is_duplicate = False
                try:
                    # Ú†Ú© Ù„ÛŒÙ†Ú©
                    params_link = {'queries[0]': f'equal("link", ["{link}"])', 'limit': 1}
                    res_link = requests.get(
                        f"{endpoint}/databases/{database_id}/collections/{collection_id}/documents",
                        headers=headers,
                        params=params_link
                    )
                    if res_link.status_code == 200:
                        data_link = res_link.json()
                        if data_link.get('total', 0) > 0:
                            is_duplicate = True
                            print(f"[SKIP] ØªÚ©Ø±Ø§Ø±ÛŒ (Ù„ÛŒÙ†Ú©): {title[:70]}")
                    # Ø§Ú¯Ø± Ù„ÛŒÙ†Ú© ØªÚ©Ø±Ø§Ø±ÛŒ Ù†Ø¨ÙˆØ¯ØŒ Ú†Ú© hash Ú©Ù†ÛŒÙ…
                    if not is_duplicate:
                        params_hash = {'queries[0]': f'equal("content_hash", ["{content_hash}"])', 'limit': 1}
                        res_hash = requests.get(
                            f"{endpoint}/databases/{database_id}/collections/{collection_id}/documents",
                            headers=headers,
                            params=params_hash
                        )
                        if res_hash.status_code == 200:
                            data_hash = res_hash.json()
                            if data_hash.get('total', 0) > 0:
                                is_duplicate = True
                                print(f"[SKIP] ØªÚ©Ø±Ø§Ø±ÛŒ (Ù…Ø­ØªÙˆØ§): {title[:70]}")
                        else:
                            print(f"[WARN] Ø®Ø·Ø§ Ø¯Ø± Ø¯Ø±Ø®ÙˆØ§Ø³Øª hash: {res_hash.status_code} - {res_hash.text}")
                    else:
                        print(f"[WARN] Ø®Ø·Ø§ Ø¯Ø± Ø¯Ø±Ø®ÙˆØ§Ø³Øª Ù„ÛŒÙ†Ú©: {res_link.status_code} - {res_link.text}")
                except Exception as e:
                    print(f"[WARN] Ø®Ø·Ø§ Ø¯Ø± Ú†Ú© ØªÚ©Ø±Ø§Ø±ÛŒ: {str(e)} - Ø§Ø¯Ø§Ù…Ù‡ Ø¨Ø¯ÙˆÙ† Ú†Ú©")
                if is_duplicate:
                    continue
                final_text = (
                    f"ğŸ’  <b>{title}</b>\n\n"
                    f"@candidatoryiran\n\n"
                    f"{description}\n\n"
                    f"ğŸ‡®ğŸ‡· ğŸ‡®ğŸ‡· ğŸ‡®ğŸ‡· ğŸ‡®ğŸ‡· ğŸ‡®ğŸ‡· ğŸ‡®ğŸ‡·\n"
                    f"Ú©Ø§Ù†Ø§Ù„ Ø®Ø¨Ø±ÛŒ Ú©Ø§Ù†Ø¯ÛŒØ¯Ø§ØªÙˆØ±ÛŒ\n"
                    f"ğŸ†” @candidatoryiran\n"
                    f"ğŸ†” Instagram.com/candidatory.ir\n"
                )

                image_url = None

                # Ø±ÙˆØ´ Û±: enclosure (Ø§Ø³ØªØ§Ù†Ø¯Ø§Ø±Ø¯)
                if 'enclosure' in entry and entry.enclosure and entry.enclosure.get('type', '').startswith('image/'):
                    image_url = entry.enclosure.href

                # Ø±ÙˆØ´ Û²: media_content (Ø§Ø³ØªØ§Ù†Ø¯Ø§Ø±Ø¯)
                elif 'media_content' in entry:
                    for media in entry.media_content:
                        if media.get('medium') == 'image' and media.get('url'):
                            image_url = media['url']
                            break

                # Ø±ÙˆØ´ Û³: media:thumbnail (Ø¨Ø³ÛŒØ§Ø± Ø±Ø§ÛŒØ¬ Ø¯Ø± Ø³Ø§ÛŒØªâ€ŒÙ‡Ø§ÛŒ Ø§ÛŒØ±Ø§Ù†ÛŒ)
                elif 'media_thumbnail' in entry and entry.media_thumbnail:
                    image_url = entry.media_thumbnail[0].get('url')

                # Ø±ÙˆØ´ Û´: Ø¯Ø§Ø®Ù„ description Ø¨Ù‡ ØµÙˆØ±Øª <img src="...">
                elif 'description' in entry:
                    desc = entry.description
                    if '<img ' in desc:
                        start = desc.find('src="') + 5
                        end = desc.find('"', start)
                        if start > 4 and end > start:
                            image_url = desc[start:end]

                # Ø±ÙˆØ´ Ûµ: Ø¯Ø§Ø®Ù„ content:encoded ÛŒØ§ content[0].value Ø¨Ù‡ ØµÙˆØ±Øª <img src="...">
                elif 'content' in entry and entry.content:
                    content = entry.content[0].get('value', '')
                    if '<img ' in content:
                        start = content.find('src="') + 5
                        end = content.find('"', start)
                        if start > 4 and end > start:
                            image_url = content[start:end]

                # Ø±ÙˆØ´ Û¶: Ø¯Ø§Ø®Ù„ media:group ÛŒØ§ media:group.media:content (Ú¯Ø§Ù‡ÛŒ Ø¯Ø± ØªØ³Ù†ÛŒÙ…/Ù…Ù‡Ø±)
                elif 'media_group' in entry:
                    for group in entry.media_group:
                        if 'media_content' in group:
                            for media in group.media_content:
                                if media.get('medium') == 'image' and media.get('url'):
                                    image_url = media['url']
                                    break

                # Ø§Ú¯Ø± Ù‡ÛŒÚ† Ø±ÙˆØ´ÛŒ Ø¹Ú©Ø³ Ù¾ÛŒØ¯Ø§ Ù†Ú©Ø±Ø¯ â†’ Ø¹Ú©Ø³ Ù¾ÛŒØ´â€ŒÙØ±Ø¶ (Ø­ØªÙ…Ø§Ù‹ Ù„ÛŒÙ†Ú© ÙˆØ§Ù‚Ø¹ÛŒ Ø¨Ú¯Ø°Ø§Ø±)
                if not image_url:
                    image_url = "https://example.com/fallback-news-image.jpg"  # â† Ù„ÛŒÙ†Ú© Ø¹Ú©Ø³ Ø«Ø§Ø¨Øª Ø®ÙˆØ¯Øª Ø±Ùˆ Ø§ÛŒÙ†Ø¬Ø§ Ø¨Ú¯Ø°Ø§Ø±
                    print(f"[FALLBACK] Ø¹Ú©Ø³ Ù¾ÛŒØ´â€ŒÙØ±Ø¶ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø´Ø¯ Ø¨Ø±Ø§ÛŒ: {title[:50]}")

                try:
                    # Ù‡Ù…ÛŒØ´Ù‡ Ø¨Ø§ send_photo Ø§Ø±Ø³Ø§Ù„ Ù…ÛŒâ€ŒØ´ÙˆØ¯ (Ø­ØªÛŒ Ø§Ú¯Ø± Ø¹Ú©Ø³ Ù¾ÛŒØ´â€ŒÙØ±Ø¶ Ø¨Ø§Ø´Ø¯)
                    await bot.send_photo(
                        chat_id=chat_id,
                        photo=image_url,
                        caption=final_text,
                        parse_mode='HTML',
                        disable_notification=True
                    )
                    posted = True
                    print(f"[SUCCESS] Ø§Ø±Ø³Ø§Ù„ Ù…ÙˆÙÙ‚: {title[:70]}")

                    # Ø°Ø®ÛŒØ±Ù‡ Ù„ÛŒÙ†Ú© Ùˆ hash Ø¨Ø§ HTTP
                    try:
                        payload = {
                            'documentId': 'unique()',
                            'data': {
                                'link': link,
                                'title': title[:300],
                                'content_hash': content_hash,
                                'created_at': now.isoformat()
                            }
                        }
                        res = requests.post(
                            f"{endpoint}/databases/{database_id}/collections/{collection_id}/documents",
                            headers=headers,
                            json=payload
                        )
                        if res.status_code in (200, 201):
                            print("[DB] Ù„ÛŒÙ†Ú© Ùˆ hash Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯")
                        else:
                            print(f"[WARN] Ø°Ø®ÛŒØ±Ù‡ Ø¯ÛŒØªØ§Ø¨ÛŒØ³ Ø´Ú©Ø³Øª: {res.status_code} - {res.text}")
                    except Exception as save_err:
                        print(f"[WARN] Ø®Ø·Ø§ Ø¯Ø± Ø°Ø®ÛŒØ±Ù‡ Ø¯ÛŒØªØ§Ø¨ÛŒØ³: {str(save_err)}")
                except Exception as send_err:
                    print(f"[ERROR] Ø®Ø·Ø§ Ø¯Ø± Ø§Ø±Ø³Ø§Ù„: {str(send_err)}")
        except Exception as feed_err:
            print(f"[ERROR] Ù…Ø´Ú©Ù„ Ø¯Ø± ÙÛŒØ¯ {url}: {str(feed_err)}")
    print(f"[INFO] Ù¾Ø§ÛŒØ§Ù† Ø§Ø¬Ø±Ø§ - Ø§Ø±Ø³Ø§Ù„ Ø´Ø¯: {posted}")
    return {"status": "success", "posted": posted}
if __name__ == "__main__":
    asyncio.run(main())
